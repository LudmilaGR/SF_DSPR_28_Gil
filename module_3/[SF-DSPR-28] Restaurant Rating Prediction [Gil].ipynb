{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://static.tacdn.com/img2/brand_refresh/Tripadvisor_lockup_horizontal_secondary_registered.svg)\n",
    "# Predict TripAdvisor Rating\n",
    "\n",
    "* Гиль Юлия\n",
    "* Группа DSPR-28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [IMPORT](#1)\n",
    "2. [DATA](#2)\n",
    "3. [CLEANING AND PREPARING DATA](#3)\n",
    "    * [Обработка NAN ](#3.1)\n",
    "    * [Обработка признаков](#3.2)\n",
    "4. [EDA](#4)\n",
    "    * [Распределение признаков](#4.1)\n",
    "    * [Распределение целевой переменной Rating](#4.2)\n",
    "    * [Корреляция признаков](#4.3)\n",
    "    * [Анализ номинативных переменных](#4.4)\n",
    "\n",
    "5. [DATA PREPROCESSING](#5)\n",
    "6. [MODEL](#6)\n",
    "7. [SUBMISSION](#7)\n",
    "8. [SUMMARY](#8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "# 1. IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/data-ta/Paris.csv\n",
      "/kaggle/input/data-ta/Budapest.csv\n",
      "/kaggle/input/data-ta/Milan.csv\n",
      "/kaggle/input/data-ta/Lisbon.csv\n",
      "/kaggle/input/data-ta/Lyon.csv\n",
      "/kaggle/input/data-ta/Rome.csv\n",
      "/kaggle/input/data-ta/Vienna.csv\n",
      "/kaggle/input/data-ta/Amsterdam.csv\n",
      "/kaggle/input/data-ta/Luxembourg.csv\n",
      "/kaggle/input/data-ta/Oporto.csv\n",
      "/kaggle/input/data-ta/Edinburgh.csv\n",
      "/kaggle/input/data-ta/Madrid.csv\n",
      "/kaggle/input/data-ta/Geneva.csv\n",
      "/kaggle/input/data-ta/Zurich.csv\n",
      "/kaggle/input/data-ta/Hamburg.csv\n",
      "/kaggle/input/data-ta/Prague.csv\n",
      "/kaggle/input/data-ta/Bratislava.csv\n",
      "/kaggle/input/data-ta/Athens.csv\n",
      "/kaggle/input/data-ta/Munich.csv\n",
      "/kaggle/input/data-ta/Stockholm.csv\n",
      "/kaggle/input/data-ta/Helsinki.csv\n",
      "/kaggle/input/data-ta/Barcelona.csv\n",
      "/kaggle/input/data-ta/Dublin.csv\n",
      "/kaggle/input/data-ta/Copenhagen.csv\n",
      "/kaggle/input/data-ta/Oslo.csv\n",
      "/kaggle/input/data-ta/Warsaw.csv\n",
      "/kaggle/input/data-ta/Berlin.csv\n",
      "/kaggle/input/data-ta/Ljubljana.csv\n",
      "/kaggle/input/data-ta/Brussels.csv\n",
      "/kaggle/input/data-ta/London.csv\n",
      "/kaggle/input/data-ta/Krakow.csv\n",
      "/kaggle/input/citiesdata-2/cities_data.csv\n",
      "/kaggle/input/sf-dst-restaurant-rating/sample_submission.csv\n",
      "/kaggle/input/sf-dst-restaurant-rating/main_task.csv\n",
      "/kaggle/input/sf-dst-restaurant-rating/kaggle_task.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from itertools import combinations\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# для рассчета расстояний между координатами\n",
    "from math import radians, sin, cos, asin, sqrt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sns.set()\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# всегда фиксируйте RANDOM_SEED, чтобы ваши эксперименты были воспроизводимы!\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зафиксируем версию пакетов, чтобы эксперименты были воспроизводимы:\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "# 2. DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Открываем необходимые данные, создаем датафреймы\n",
    "DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "df_train = pd.read_csv(DATA_DIR+'main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR+'kaggle_task.csv')\n",
    "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')\n",
    "cities_info = pd.read_csv('/kaggle/input/citiesdata-2/cities_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим датафрейм data_ta из данных, которые собрали с TripAdvisor\n",
    "# Открываем каждый файл и добавляем его в data_ta\n",
    "data_ta = pd.DataFrame()\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/data-ta/'):\n",
    "    for filename in filenames:\n",
    "        temp = pd.read_csv(os.path.join(dirname, filename))\n",
    "        data_ta = pd.concat([data_ta, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1  # помечаем где у нас трейн\n",
    "df_test['sample'] = 0  # помечаем где у нас тест\n",
    "# в тесте у нас нет значения Rating, мы его должны предсказать, поэтому пока просто заполняем нулями\n",
    "df_test['Rating'] = 0\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(\n",
    "    drop=True)  # объединяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее по признакам:\n",
    "* City: город \n",
    "* Cuisine Style: кухня\n",
    "* Ranking: ранг ресторана относительно других ресторанов в этом городе\n",
    "* Price Range: цены в ресторане в 3 категориях\n",
    "* Number of Reviews: количество отзывов\n",
    "* Reviews: 2 последних отзыва и даты этих отзывов\n",
    "* URL_TA: страница ресторана на 'www.tripadvisor.com' \n",
    "* ID_TA: ID ресторана в TripAdvisor\n",
    "* Rating: рейтинг ресторана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Reviews[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, большинство признаков у нас требует очистки и предварительной обработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "# 3. CLEANING AND PREPARING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1. Обработка NAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим вцелом на наличие пустых значений и определим стратегию работы с ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intitial_eda_checks(df, missing_percent):\n",
    "    '''\n",
    "    Функция принимает на вход датафрейм, а также заданный порог % пустых значений, который хотим обработать. \n",
    "    На выход выводит на экран информацию о сумме пустых значений для всех колонок, а также проце\n",
    "    '''\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        mask_total = df.isnull().sum().sort_values(ascending=False)\n",
    "        total = mask_total[mask_total > 0]\n",
    "\n",
    "        mask_percent = df.isnull().mean().sort_values(ascending=False)\n",
    "        percent = mask_percent[mask_percent > 0]\n",
    "\n",
    "        series = mask_percent[mask_percent > missing_percent]\n",
    "        columns = series.index.to_list()\n",
    "\n",
    "        missing_data = pd.DataFrame(pd.concat(\n",
    "            [total, round(percent*100, 2)], axis=1, keys=['Количество', '%']))\n",
    "        print('Сумма и процент значений NaN:\\n \\n')\n",
    "        display(missing_data)\n",
    "    else:\n",
    "        print('NaN значения не найдены.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем функцию вывода всех пустых значений\n",
    "intitial_eda_checks(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, как распределены пропуски\n",
    "sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В таблице выведена информация по всем пустым значениям для всех столбцов основного рабочего датасета (data). \n",
    "\n",
    "* В 4 из 10 столбцов присутствуют пропуски. \n",
    "* В столбцах Price Range и Cuisine Style очень большое количество пропусков. \n",
    "* По условию задания строки мы не удаляем, пробуем заменить.\n",
    "\n",
    "Давайте пройдем по каждому из признаков, где есть пустые значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Cuisine Style\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущено 23.18% данных. \n",
    "\n",
    "Пропуски попробуем заполнить данными с Trip Advisor, но сделаем это позже, в секции формирования новых признаков (см. раздел [3.2 Обработка признаков](#3.2). Пропуски, которые заполнить не удастся реальными данными, заменим на значение 'Unknown'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Price Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущено 34.72% данных. \n",
    "\n",
    "Посмотрим на распределение признака, группировку по городу, рангу, продумаем варианты заполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительный просмотр данных\n",
    "data['Price Range'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы по признаку:** 70% кухонь - средней ценовой категории\n",
    "\n",
    "**Стратегия заполнения:**\n",
    "1. Найти данные на внешних ресурсах\n",
    "2. Значением моды."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполним пропуски модой\n",
    "data['Price Range'].fillna(data['Price Range'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Number of Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущено 6.40% данных. \n",
    "\n",
    "Посмотрим на распределение признака, группировку по городу, продумаем варианты заполнения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предварительный просмотр данных\n",
    "data['Number of Reviews'].hist()\n",
    "data['Number of Reviews'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение по городам\n",
    "data.groupby(['City'])[\n",
    "    'Number of Reviews'].agg(['max', 'min', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы по признаку:**\n",
    "* Есть выбросы => сильное влияние на среднее по всему датасету.\n",
    "* У 50% данных количество ревью от 7 до 105. Медиана - 28\n",
    "* Есть зависимость среднего/медианы от города. \n",
    "\n",
    "**Стратегия заполнения:**\n",
    "1. Найти данные на внешних ресурсах\n",
    "2. Медианным значением в зависимости от города\n",
    "3. Заполнить нулями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отсутствие данного значения может быть важной информацией для модели. \n",
    "\n",
    "Поэтому давайте вынесем все пропуски в отдельный признак (number_of_rev_is_NAN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новый признак\n",
    "data['number_of_rev_is_NAN'] = pd.isna(\n",
    "    data['Number of Reviews']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропуски медианой по городу\n",
    "# series с медианами по городам\n",
    "median_reviews = data.groupby(['City'])['Number of Reviews'].median()\n",
    "data['Number of Reviews'] = data.apply(lambda x: median_reviews.loc[x['City']] if pd.isna(\n",
    "    x['Number of Reviews']) else x['Number of Reviews'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков очень мало. \n",
    "\n",
    "Обработку пустых значений добавила в раздел формирования новых признаков (см. раздел [3.2 Обработка признаков](#3.2))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим еще раз на информацию по пропускам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем функцию вывода всех пустых значений\n",
    "intitial_eda_checks(data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "## 3.2. Обработка признаков\n",
    "\n",
    "#### Поиск категориальных признаков для обработки\n",
    "Для начала посмотрим, какие признаки могут быть категориальными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Предварительно, категориальными выглядят:\n",
    "* City\n",
    "* Price Range\n",
    "* Cuisine Style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Чистка признака Restaurant_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признак Restaurant_id cодержит id_ перед номером, избавимся от префикса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Restaurant_id.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Почистим формат колонок с ID\n",
    "data.Restaurant_id = data.Restaurant_id.apply(lambda x: int(x[3:]))\n",
    "data.ID_TA = data.ID_TA.apply(lambda x: int(x[1:]))\n",
    "data.Restaurant_id.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных с TripAdvisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный с TripAdvisor (TA) были получены через решение https://apify.com/maxcopell/tripadvisor#api-usage на платформе APIFY. \n",
    "\n",
    "Залиты и добавлены в датафрейм data_ta.\n",
    "\n",
    "С данных TA нам понадобятся точно:\n",
    "* Список кухонь\n",
    "* Колисество наград у ресторана\n",
    "* Информация по расположению ресторана (широта и долгота).\n",
    "\n",
    "Если позволит время, то дополинтельно можно использовать информацию для заполнения пропусков:\n",
    "* Price Range\n",
    "* Number of Reviews.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Чистка данных**\n",
    "\n",
    "Почистим пока ненужные колонки, сделаем преобразования датафрейма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаляем ненужные колонки\n",
    "# Для списка сертификатов будем использовать только имя сертификата\n",
    "data_ta.drop([col for col in data_ta.columns if col.endswith(\n",
    "    '/year')], axis=1, inplace=True)\n",
    "data_ta.drop([col for col in data_ta.columns if col.startswith(\n",
    "    'hours/')], axis=1, inplace=True)  # Время работы ресторана не используем\n",
    "data_ta.drop(['address', 'phone', 'rankingPosition', 'type', 'webUrl', 'website', 'email',\n",
    "              'isClosed', 'isLongClosed', 'rating'], axis=1, inplace=True)  # Доп список признаков, которые решила не использовать точно\n",
    "\n",
    "data_ta.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переименуем колонку ID_TA для простоты последующего мержа\n",
    "data_ta.rename(columns={\"id\": \"ID_TA\"}, inplace=True)\n",
    "data_ta.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Т.к. набор колонок у разных испточников данных разный, то отсортируем все полученные колонки по алфавиту\n",
    "# для простоты работы и воспрозводимости кода\n",
    "# переменная со списком отсортированных колонок\n",
    "columns_list = list(data_ta.columns.sort_values())\n",
    "data_ta = data_ta[columns_list]  # модифицируем датафрейм\n",
    "data_ta.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создадим новые признаки:**\n",
    "1. awards_ta - список с наградами\n",
    "2. awards_num - количество наград у ресторана\n",
    "2. cuisine_styles_ta - список с кухнями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем признак, который будет хранить все награды ресторана\n",
    "data_ta['awards_ta'] = data_ta[data_ta.columns[1:12]].apply(\n",
    "    lambda x: ', '.join(x.dropna().astype(str)),\n",
    "    axis=1)  # Проходимся по колонкам с наградами, объединяем непустые значения в строку через запятую\n",
    "\n",
    "data_ta['awards_ta'] = data_ta['awards_ta'].apply(\n",
    "    lambda x: x.split(\", \"))  # создаем список наград для каждого ресторана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем признак awards_num, который будет хранить количество наград у ресторана\n",
    "len_cert_list = []\n",
    "\n",
    "for i in range(0, len(data_ta)):\n",
    "    if data_ta['awards_ta'][i][0] == '':  # если список наград пустой, то записываем 0\n",
    "        len_cert_list.append(0)\n",
    "    else:\n",
    "        # если непустой, то записываем длину списка\n",
    "        len_cert_list.append(len(data_ta['awards_ta'][i]))\n",
    "\n",
    "data_ta['awards_num'] = len_cert_list  # добавляем признак"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем признак со списками кухонь, который будет хранить список кухонь дл] ресторана\n",
    "data_ta['cuisine_styles_ta'] = data_ta[data_ta.columns[13:-9]].apply(\n",
    "    lambda x: ', '.join(x.dropna().astype(str)),\n",
    "    axis=1)  # Проходимся по колонкам с кухнями, объединяем непустые значения в строку через запятую\n",
    "\n",
    "data_ta['cuisine_styles_ta'] = data_ta['cuisine_styles_ta'].apply(\n",
    "    lambda x: x.split(\", \"))  # создаем список кухонь для каждого ресторана"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем итоговый датафрейм с внешними данными, который будем использовать дальше для генерации признаков (data_ta_output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем датафрейм с колонками, которые хотим перенести в исходный датафрейм для модели data\n",
    "data_ta_output = data_ta[['ID_TA', 'awards_num',\n",
    "                          'cuisine_styles_ta', 'longitude', 'latitude']]\n",
    "# удаляем дубликаты для ресторанов (такие есть) для корректного мержа\n",
    "data_ta_output.drop_duplicates(subset=['ID_TA'], inplace=True)\n",
    "data_ta_output.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смержим рабочий датафрейм с внешними данными из TA\n",
    "data = pd.merge(data, data_ta_output, on=\"ID_TA\",\n",
    "                how=\"left\")  # объединяем по ID_TA\n",
    "\n",
    "data.sample(1)\n",
    "data.info()  # Проверим, что количество строк осталось прежним"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внешние данные добавили. \n",
    "\n",
    "К сожалению, инструмент, который использовала, не предоставил исчерпывающую базу данных, поэтому по добавленным признакам есть пропуски (есть 41247 из 50000). Будем пропуски обрабатывать далее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак AWARDS_NUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавили новый признак с TA про количество наград.\n",
    "\n",
    "Посмотрим на распределение признака awards_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.awards_num.hist(bins=11)\n",
    "data.awards_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на пустые значения по этому признаку (17.5%) и заменим их на медиану, т.е. нули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызовим функцию по просмотру NA значений\n",
    "intitial_eda_checks(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем медианой NA в awards_num\n",
    "data.awards_num.fillna(data.awards_num.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак CITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в нашу выборку новые признаки по городам (данные из интеренета):\n",
    "* country - страна, в которой находится город\n",
    "* citizens - население города, чел\n",
    "* restaurants_number_TA - количество ресторанов, участвующих в рейтинге (TripAdvisor)\n",
    "* citizens_per_restaurant - количество горожан на один ресторан\n",
    "* tourists_per_year - количество туристов, посетивших город в течение года, чел\n",
    "* ttl_ppl_per_restaurants - (количество туристов + население города) / количество ресторанов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Мержим два датафрейма\n",
    "data = pd.merge(data, cities_info, on=\"City\", how=\"left\")  # объединяем по City\n",
    "data.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, сколько уникальных городов\n",
    "data.City.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей выборке 31 город. Не очень много.\n",
    "Для данного признака попробуем dymmy-кодирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим признак с копией городов перед кодированием, т.к. изначальная колонка может быть полезной.\n",
    "data['city_copies'] = data['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем One-Hot Encoding в pandas - get_dummies для кодирования городов.\n",
    "data = pd.get_dummies(data, columns=['City', ], dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак DISTANCE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На основе внешних данных созададим новый признак distance, который будет показывать расстояние от центра города до ресторана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    '''\n",
    "    Функция принимает на вход координаты города и ресторана. \n",
    "    На выходе возвращает расстояние от центра города до ресторана.\n",
    "    '''\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2) ** 2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    earth_radius = 6371  # in km\n",
    "    return c * earth_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новый признак distance\n",
    "data['distance'] = data.apply(lambda row:\n",
    "                              haversine(lon1=row['lon_c'],\n",
    "                                        lat1=row['lat_c'],\n",
    "                                        lon2=row['longitude'],\n",
    "                                        lat2=row['latitude']),\n",
    "                              axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На предыдущих этапах мы выяснили, что не для всех ресторанов из внешних иточников была информацию о широте и долготе. \n",
    "Поэтому нужно не забыть заполнить пропуски. Заполним значением среднего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем пропуски значением среднего по городу\n",
    "mean_distance = data.groupby(['city_copies'])['distance'].mean()\n",
    "data['distance'] = data.apply(lambda x: mean_distance.loc[x['city_copies']] if pd.isna(\n",
    "    x['distance']) else x['distance'], axis=1)\n",
    "\n",
    "# data['distance'].fillna(data['distance'].mean(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какое распределение признака получилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['distance'].hist()\n",
    "data['distance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак NUMBER OF REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Число отзывов сильно влияет на ранг/рейтинг. При этом вр время EDA ниже было выявлено, что разброс значений очень большой.\n",
    "\n",
    "Попробуем создать новый признак:\n",
    "*  reviews_per_ttl_ppl - показывает сколько ревью приходится на суммарное 1000 людей (жители + туристы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новый признак с использованием внешних данных по городам\n",
    "data['reviews_per_ttl_ppl'] = data.apply(lambda row: (\n",
    "    row['Number of Reviews']/(row['citizens']+row['tourists_per_year']))*1000, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какое распределение признака получили."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reviews_per_ttl_ppl'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак PRICE RANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие значени содержит признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Price Range'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По описанию 'Price Range' это - цены в ресторане. Их можно поставить по возрастанию (значит это не категориальный признак). А это значит, что их можно заменить последовательными числами.\n",
    "\n",
    "Price_range можно разбить на числовой признак от 1 до 3:\n",
    "* Низкий уровень цен - 1 \n",
    "* Средний ценовой сегмент - 2 \n",
    "* Высокий уровень цен - 3\n",
    "\n",
    "Код ниже создаёт новый признак price_range_num."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем словать с кодировкой значений в числовые\n",
    "pricerange_dict = {\"nan\": 0, \"$\": 1, \"$$ - $$$\": 2, \"$$$$\": 3}\n",
    "data['price_range_num'] = data['Price Range']\n",
    "data['price_range_num'].replace(\n",
    "    to_replace=pricerange_dict, inplace=True)  # заменяем значения в соответствии со словарем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак CUISINE STYLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на содержание этого столбца. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество уникальных значений\n",
    "data['Cuisine Style'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Примеры данных\n",
    "data['Cuisine Style'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тип данных\n",
    "type(data['Cuisine Style'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим данные в столбце."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_name(str_val):\n",
    "    \"\"\"\n",
    "    Преобразует строку с названиями кухонь в список [list] названий кухонь.\n",
    "    На входе:\n",
    "        - строковая переменная, содержащая названия кухонь.\n",
    "    На выходе:\n",
    "        - список [list] названий кухонь.\n",
    "    \"\"\"\n",
    "    if pd.isna(str_val):\n",
    "        return ['Unknown']\n",
    "    str_val = str_val.strip('[]')  # Отбрасываем скобки\n",
    "    str_val = str_val.replace(\"\\'\", '')  # Убираем кавычки '\n",
    "    str_val = str_val.split(\", \")  # Разбиваем строку по названиям кухонь\n",
    "    return str_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применим ф-ию по чистке данных\n",
    "data[\"Cuisine Style\"] = data[\"Cuisine Style\"].apply(clean_name)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помним, что список кухонь содержит 23.18% пропусков. При чистке мы заменили их на \"Unknown\". Для данных, по которым нашли информацию на TA, сделаем замену."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuisine_nan_replace(row):\n",
    "    '''\n",
    "    Функция на вход принимает строку датафрейма, проверяем ее значение.\n",
    "    На выход возвращает или изначальное значение списка кухонь, или соотвествующий список с TA для тех кухонь, где указано Unknown.\n",
    "    '''\n",
    "    if row['Cuisine Style'][0] == 'Unknown':\n",
    "        return row['cuisine_styles_ta']\n",
    "    else:\n",
    "        return row['Cuisine Style']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем данными с TA с помощью функции\n",
    "data['Cuisine Style'] = data.apply(cuisine_nan_replace, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intitial_eda_checks(data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После заполнения данными с TA осталось 5% пропусков. Заполним их значением Unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем значением Unknown\n",
    "data['Cuisine Style'].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строки с Unknown типа str, а остальные - list. Сделаем преобразования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_type(str_val):\n",
    "    \"\"\"\n",
    "    Преобразует строку с Unknown названием кухни в список [list].\n",
    "    На входе:\n",
    "        - колонка, содержащая названия кухонь.\n",
    "    На выходе:\n",
    "        - список [list] названий кухонь.\n",
    "    \"\"\"\n",
    "    if type(str_val) == str:\n",
    "        return str_val.split()\n",
    "    return str_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем функцию\n",
    "data[\"Cuisine Style\"] = data[\"Cuisine Style\"].apply(clean_type)\n",
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проанализируем, какое количество кухонь в среднем у ресторнов.\n",
    "\n",
    "Среднее количество кухонь у ресторана - 2.6 (до заполнения данными с TA), 2.7 - после."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посчитаем среднее количество кухонь у ресторана\n",
    "count = 0\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    count += len(data['Cuisine Style'][i])\n",
    "\n",
    "round(count/len(data), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также посмотрим, сколько уникальных кухонь.\n",
    "\n",
    "Всего 125 уникальных кухонь (до заполнения с TA), 146 - после. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_explode(df, col, cnt=False):\n",
    "    \"\"\"\n",
    "    Принимает на входе объект DataFrame df и 'имя' столбца col.\n",
    "    Если cnt = True (\"режим value_counts\"):\n",
    "        - возвращает объект series типа value_counts для столбца col.\n",
    "    Если cnt = False (\"режим DataFrame\"):\n",
    "        - возвращает объект DataFrame c \"разъединёнными\" элементами столбца col\n",
    "    \"\"\"\n",
    "    df = df.explode(col)\n",
    "    if cnt:\n",
    "        return df[col].value_counts()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, сколько уникальных названий кухни, применив ф-ию.\n",
    "cuisine_count = data.copy()  # создадим копию датафрейма\n",
    "data_explode(cuisine_count, 'Cuisine Style', cnt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создали отдельный df с кухнями в режиме explode\n",
    "cuisine_count = data_explode(cuisine_count, 'Cuisine Style', cnt=False)\n",
    "cuisine_count.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сразу посмотрим на распределение признака\n",
    "(cuisine_count[\"Cuisine Style\"].value_counts()).hist(bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что всего представлено 146 уникльных кухонь. При этом подавляющее большинство (2/3) упоминается не так часто, но и есть особо популярные кухни. Можно подумать про объединение кухонь по частоте упоминания.\n",
    "\n",
    "Пока сформируем признак cuisine_num, который будет показывать, сколько типов кухонь представлено у ресторана."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем признак cuisine_num\n",
    "len_cuisines_list = []\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    if data['Cuisine Style'][i][0] == 'Unknown':\n",
    "        len_cuisines_list.append(-1)  # -1 для пропуско\n",
    "    elif data['Cuisine Style'][i][0] == '':\n",
    "        len_cuisines_list.append(0)  # 0, где кухонь нет и на TA\n",
    "    else:\n",
    "        len_cuisines_list.append(len(data['Cuisine Style'][i]))\n",
    "\n",
    "data['cuisine_num'] = len_cuisines_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение признака\n",
    "data['cuisine_num'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ресторанов, для которых были пропуски и не заполнились данными с TA, заменим значение количества кухонь на медианное.\n",
    "\n",
    "Такиех записей 2442.\n",
    "\n",
    "Закомментировала, т.к. MAE при таком подходе хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['cuisine_num'].replace(-1, data['cuisine_num'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По ходу проведения EDA было замечено, что определенные опции влияют на рейтинг/количество ревью.\n",
    "\n",
    "У категории кухонь, отмеченных на TA, как Dietary Restrictions рейтинги выше. \n",
    "Можно отметить рестараны отдельным признаком, если такие опции у него имеются.\n",
    "Какие опции включаем:\n",
    "* Vegetarian Friendly\n",
    "* Vegan Options\n",
    "* Halal\n",
    "* Kosher\n",
    "* Gluten Free Options. \n",
    "\n",
    "Выделим в отдельный признак dietary_restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dietary_restrictions(row):\n",
    "    \"\"\"\n",
    "    Функция на вход принимает строку датафрейма.\n",
    "    Если в списке кухонь ресторана есть одна из кухонь списка спец. кухонь, то\n",
    "        - возвращаем 1\n",
    "        - иначе возвращаем 0.\n",
    "    \"\"\"\n",
    "    dietary_restrictions = ['Vegetarian Friendly', 'Vegan Options',\n",
    "                            'Gluten Free Options', 'Halal', 'Kosher']\n",
    "    for i in dietary_restrictions:\n",
    "        if i in row['Cuisine Style'] and i != '':\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем признак dietary_restrictions\n",
    "data['dietary_restrictions'] = data.apply(dietary_restrictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И последний шаг - создание дамми-признаков для всех кухонь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем MultiLabelBinarizer() для кодирования\n",
    "\n",
    "s = data['Cuisine Style']\n",
    "mlb = MultiLabelBinarizer()\n",
    "cuisine_df = pd.DataFrame(mlb.fit_transform(\n",
    "    s), columns=mlb.classes_, index=data.index)  # cсоздаем датафрейм с дамми кухнями\n",
    "\n",
    "cuisine_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смержим рабочий датафрейм с датафреймом дамми-кухонь\n",
    "data = data.merge(cuisine_df, left_index=True, right_index=True)\n",
    "data.info()\n",
    "data.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак REVIEWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на reivews. Видим, что он содержит два ревью с датами ревью.\n",
    "Мы можем вытащить несколько новых признака из дат:\n",
    "* review_date: все даты ревью\n",
    "* date_rev_1: дата первого ревью\n",
    "* date_rev_2: дата второго ревью\n",
    "* date_rev_delta: количество дней между оставленными ревью\n",
    "* date_rev_from_max: количество дней от последнего отзыва до самого свежего отзыва в датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, что содержится в столбце с ревью.\n",
    "data.Reviews[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тип данных - str\n",
    "type(data.Reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В тестовой выборке есть пустые значения, заменим их на строку, которая показывает, что ревью нет.\n",
    "data['Reviews'].fillna('[[], []]', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим новый признак review_date на основе патерна поиска дат.\n",
    "pattern = re.compile('\\d+\\/\\d+\\/\\d+')\n",
    "data['review_date'] = data.Reviews.apply(pattern.findall)\n",
    "\n",
    "data['review_date'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что review_date может содержать одну дату, две даты, три даты, ни одной даты.\n",
    "\n",
    "Кодом ниже проверим, есть ли такие ревью, где дата содержалась в самом комментарии и создался список из трёх дат. Да, такие записи есть.\n",
    "Применим к таким полям функцию, которая первое упоминание из комментариев почистит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Напечатать даты, где более двух дат\n",
    "for i in range(0, len(data)):\n",
    "    if len(data.review_date[i]) > 2:\n",
    "        print(i, len(data.review_date[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чистка данных, где в поле review_date попали даты-упоминания из комментариев отзыва.\n",
    "data.review_date = data.review_date.apply(\n",
    "    lambda x: [x[-2], x[-1]] if len(x) > 2 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, сколько данных, где менее двух отзывов.\n",
    "count = 0\n",
    "for i in range(0, len(data)):\n",
    "    if len(data.review_date[i]) < 2:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим признаки:\n",
    "* date_rev_1: дата первого ревью\n",
    "* date_rev_2: дата второго ревью\n",
    "* date_rev_delta: количество дней между оставленными ревью\n",
    "* date_rev_from_max: количество дней от последнего отзыва до самого свежего отзыва в датасете.\n",
    "    \n",
    ">     Информация с TA: Свежие отзывы имеют большую ценность, чем написанные давно. Они дают более точное представление о том, чего в данный момент стоит ожидать от компании. Это значит, что отзывы, которые были написаны давно (независимо от того, положительные они или отрицательные), имеют меньший вес при расчете рейтинга компании, чем отзыв, написанный недавно. Несмотря на то, что устаревшие отзывы не имеют такого же веса в рейтинге, они по-прежнему отображаются в разделе \"Обзор\" на странице каждого объекта в каталоге и в истории отзывов о компании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новые признаки, сразу переводим в формат datetime64\n",
    "data['date_rev_1'] = pd.to_datetime(\n",
    "    data.review_date.apply(lambda x: x[0] if len(x) >= 1 else None))\n",
    "data['date_rev_2'] = pd.to_datetime(\n",
    "    data.review_date.apply(lambda x: x[1] if len(x) >= 2 else None))\n",
    "data['date_rev_delta'] = (\n",
    "    abs(data.date_rev_2-data.date_rev_1)) / np.timedelta64(1, \"D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальная дата отзывы в датасете\n",
    "date_max = data[['date_rev_1', 'date_rev_2']].max(axis=1).max()\n",
    "date_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новый признак про актуальность отзывов\n",
    "data['date_rev_from_max'] = data.apply(lambda row: None if len(row.review_date) == 0  # если пустые значения, то Nan\n",
    "                                       # если одна дата, то смотрим разницу с первым отзывом\n",
    "                                       else (date_max-row.date_rev_1) if len(row.review_date) == 1\n",
    "                                       else ((date_max-row.date_rev_2)), axis=1) / np.timedelta64(1, \"D\")  # если два отзыва, то берем второй отзыв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Restaurant_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на количество уникальный ID из 50 000 записей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество уникальных ID\n",
    "data.Restaurant_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из 50000 записей только 13094 уникальных ID:\n",
    "* 3807 - рестораны, представленные одним заведением\n",
    "* Остальные 46193 - сетевые рестораны.\n",
    "\n",
    "Создадим новый признак \"in chain\", который будет \n",
    "* 0 - если ресторан несетевой, \n",
    "* 1 - ресторан сетевой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Найдем ID ресторанов, у которых в value_counts более одного ресторана, сохраним список\n",
    "in_chain_index = data['Restaurant_id'].value_counts(\n",
    ").loc[lambda x: x > 1].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# запишем ID ресторанов, у кого value_counts > 1\n",
    "data['in_chain'] = data['Restaurant_id'].apply(\n",
    "    lambda x: 1 if x in in_chain_index else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на получившиеся значения\n",
    "data['in_chain'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "# 4. EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "\n",
    "### 4.1 Распределение признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции для отрисовки графиков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplot_2(column):\n",
    "    \"\"\"\n",
    "    Функция для отрисовки коробочной диаграммы для нечисловых величин.\n",
    "    На вход получаем список колонок для отрисовки. \n",
    "    Отрисовываем относительно целевой переменной Rating.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 4))\n",
    "    sns.boxplot(x=column, y='Rating',\n",
    "                data=data[data['sample'] == 1],\n",
    "                ax=ax)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.set_title('Boxplot для ' + column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Служебные признаки\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки, которые не анализируем и которые удалим перед отправкой данных на обучение модели:\n",
    "* URL_TA — URL страницы ресторана на TripAdvisor;\n",
    "* ID_TA — идентификатор ресторана в базе данных TripAdvisor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Restaurant_id -> In_Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы сгенерировали признак \"in chain\", посмотрим на него."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение признака\n",
    "plt.rcParams['figure.figsize'] = (2, 5)\n",
    "data['in_chain'].hist(bins=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение других признаков от того, является ли ресторан сетевым."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение других признаков от того, является ли ресторан сетевым\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"in_chain\", y=\"Rating\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"in_chain\", y=\"Number of Reviews\", ax=ax[1])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"in_chain\", y=\"price_range_num\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство ресторанов - сетевые. Наблюдается зависимость количества отзывов от того, сетевой ли ресторан. У сетевых ресторанов отзывов больше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "df_train['Ranking'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['City'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как изменится распределение в большом городе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['City'] == 'London'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на топ 10 городов\n",
    "for x in (df_train['City'].value_counts())[0:10].index:\n",
    "    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n",
    "\n",
    "Чтобы скорректировать признак и сделать его более информативным создадим новый признак: \n",
    "* rank_per_ttl - показывает относительную позицию ранга ресторана к общему количеству рангов по городу. Предположила, что количетсво рангов по городу будет близко к общему количеству ресторанов по версии TA.\n",
    "    Чем больше ранг - тем лучше. Чем меньше rank_per_ttl - тем лучше с позиции рейтинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем признак rank_per_ttl\n",
    "data['rank_per_ttl'] = data.apply(\n",
    "    lambda x: x['Ranking']/x['restaurants_number_TA'], axis=1)\n",
    "data.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смотрим распределение\n",
    "data['rank_per_ttl'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение других признаков относительно нового признака\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"rank_per_ttl\", y=\"Rating\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"Number of Reviews\", y=\"rank_per_ttl\", ax=ax[1])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"price_range_num\", y=\"rank_per_ttl\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сравнения посмотрим на паспределение изначального Ranking к целевой переменной Rating и другими признаками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"Ranking\", y=\"Rating\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"Number of Reviews\", y=\"Ranking\", ax=ax[1])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"price_range_num\", y=\"Ranking\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что новый признак имеет более выраженную зависимость с целевой переменной Rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Графики:**\n",
    "1. чем меньше ранг ресторана, тем чаще встречается более высокий рейтинг. Наличие корреляции с целевым признаком - хорошо для обучения модели.\n",
    "2. чем меньше ранг ресторана, тем большее количество отзывов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим признаки перемножением двух скоррелированных признаков с Ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавление признаков\n",
    "data[\"ranking_num_reviews\"] = data[\"Ranking\"] * data[\"Number of Reviews\"]\n",
    "data[\"ranking_num_cuisines\"] = data[\"Ranking\"] * data[\"cuisine_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Price Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем использовать для анализа новый признак price_range_num, т.к. он полностью дублирует изначальный признак. Пропуски уже заполнены модой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (2, 5)\n",
    "data['price_range_num'].hist(bins=3)\n",
    "data['price_range_num'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**: большая часть ресторанов средней ценовой категории. Самая немногочисленная часть - дорогие рестораны."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение рейтингов по ценовым категориям.\n",
    "get_boxplot_2('price_range_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поссмотрим зависимость целевой переменной и ценовой категории на тестовой части выборки\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"price_range_num\", y=\"Rating\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"Number of Reviews\", y=\"price_range_num\", ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Зависимость целевой переменной и ценовой категории**:\n",
    "1. Как низкие, так и высокие рейтинги представлены во всех ценовых категориях\n",
    "2. Наиболее разнообразное распределение рейтингов представлено во 2-ой ценовой категории. По 1-ой и 2-ой значения очень похожи.\n",
    "\n",
    "Максимальная корреляция ценовой категории с количеством отзывов. Чем категория выше, тем больше отзывов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Number of Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение признака, на зависимость признака от целевой переменной и других переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10, 7)\n",
    "df_train['Number of Reviews'].hist(bins=70)\n",
    "df_train['Number of Reviews'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_train, x=\"Number of Reviews\", y=\"Rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в случае с Ranking, количество отзывов очень отличается от ресторана/города (население, поток туристов).\n",
    "\n",
    "Чтобы получить более информативную картину об отзывах ресторана, создадим новый признак:\n",
    "* ttl_reviews_per_city - суммарное количество  ревью по городу из выборки\n",
    "* reviews_perc_in_city_ttl - отношения количества ревью ресторана к суммарному количеству ревью по городу из выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим датафрейм, в который запишем суммы количества ревью по городам\n",
    "reviews_sum = pd.DataFrame(data.groupby(['city_copies'])[\n",
    "    'Number of Reviews'].sum().sort_values(ascending=False))\n",
    "reviews_sum.rename(\n",
    "    columns={\"Number of Reviews\": \"ttl_reviews_per_city\"}, inplace=True)\n",
    "reviews_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Смержим созданный датафрейм с исходным датафреймам по городу\n",
    "data = pd.merge(data, reviews_sum, on=\"city_copies\", how=\"left\")\n",
    "data.sample(1)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем новый признак reviews_perc_in_city_ttl\n",
    "data['reviews_perc_in_city_ttl'] = data.apply(\n",
    "    lambda x: x['Number of Reviews']/x['ttl_reviews_per_city'], axis=1)\n",
    "data.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По распределению изначальных данных количества ревью видим, что есть выбросы, устраним их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_analysis(series, mode=False):\n",
    "    \"\"\"\n",
    "    Функция выводит инфорамцию о границах выборосов для признака.\n",
    "    Если mode = True, возвращается верхняя и нижняя границы выбросов.\n",
    "    \"\"\"\n",
    "    IQR = series.quantile(0.75) - series.quantile(0.25)\n",
    "    perc25 = series.quantile(0.25)\n",
    "    perc75 = series.quantile(0.75)\n",
    "\n",
    "    f = perc25 - 1.5*IQR\n",
    "    l = perc75 + 1.5*IQR\n",
    "\n",
    "    if mode:\n",
    "        return f, l\n",
    "\n",
    "    print(\n",
    "        '25-й перцентиль: {},'.format(perc25),\n",
    "        '75-й перцентиль: {},'.format(perc75),\n",
    "        \"IQR: {}, \".format(IQR),\n",
    "        \"Границы выбросов: [{f}, {l}].\".format(f=perc25 - 1.5*IQR, l=perc75 + 1.5*IQR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию для расчета границ выбросов для всех данных\n",
    "iqr_analysis(data['Number of Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на границы выбросов по городам\n",
    "cols = [\"lower_border\", \"higher_border\"]\n",
    "lst = []\n",
    "\n",
    "for x in (df_train['City'].value_counts()).index:\n",
    "    lst.append(iqr_analysis(\n",
    "        df_train['Number of Reviews'][df_train['City'] == x], mode=True))\n",
    "\n",
    "reviews_IQ = pd.DataFrame(lst, columns=cols)\n",
    "reviews_IQ['city'] = df_train['City'].value_counts().index\n",
    "\n",
    "display(reviews_IQ)\n",
    "print('Максимальное значение среди городов по верхней границе выбросов:',\n",
    "      reviews_IQ.higher_border.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы**:\n",
    "* Выглядит так, что в количестве отзывов есть выбросы.\n",
    "* Границы выбросов варьируются от города к городу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение признака до максимальной границы выбраса\n",
    "df_train[df_train['Number of Reviews'] <\n",
    "         reviews_IQ.higher_border.max()]['Number of Reviews'].hist(bins=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменяем выбросы на 840."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество выбросов при границе в 840\n",
    "len(data[data['Number of Reviews'] > 840]['Number of Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим выбросы в датафрейме data на максимальное пограничное знаечение признака\n",
    "print('Будет заменено записей:', len(\n",
    "    data[data['Number of Reviews'] > 840]['Number of Reviews']))\n",
    "data['Number of Reviews'] = data['Number of Reviews'].apply(\n",
    "    lambda x: 840 if x >= 840 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"Number of Reviews\", y=\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.boxplot(x='Rating', y='Number of Reviews',\n",
    "            data=data[data['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для Reviews % in City/Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение признака, на зависимость признака от целевой переменной и других переменных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределению ресторанов по городам в %\n",
    "display(pd.DataFrame(\n",
    "        data['city_copies'].value_counts(normalize=True)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение рейтингов по городам.\n",
    "get_boxplot_2('city_copies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.boxplot(x='city_copies', y='Number of Reviews',\n",
    "            data=data[data['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для city_copies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.boxplot(x='city_copies', y='price_range_num',\n",
    "            data=data[data['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для city_copies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.boxplot(x='city_copies', y='date_rev_delta',\n",
    "            data=data[data['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для city_copies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 4))\n",
    "sns.boxplot(x='city_copies', y='Ranking',\n",
    "            data=data[data['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для city_copies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Выводы по графику**:\n",
    "1. Зависимость с целевой переменной:\n",
    "    * Медиана по всем городам совпадает (4), кроме Милана\n",
    "    * Кухня Милана самая низко-оцениваемая\n",
    "    * Распределение рейтингов двух типов: а) от 3.5-4.5 с длинным хвостом до 2 b) 4-4.5 с коротким хвостом до 3.5.\n",
    "\n",
    "    Нельзя формировать новые признаки на базе целевой переменной, но хорошо бы найти закономерность в разбиении городов на 2-3 группы.\n",
    "\n",
    "2. Распределение рангов и городов тоже очень отличается. Выглядит так, что ранг завязан на количество ресторанов в городе. Чем больше ресторанов, тем размашистее распределение рангов между городами.\n",
    "3. Дельта между двумя последними отзывами по городам практически идентична по своему распределению.\n",
    "4. Количество ревью имеет больший размах для более туристических городов.\n",
    "\n",
    "**Идеи по генерации новых признаков**:\n",
    "1. Посмотреть доп. признаки по городам: население, общее количество ресторанов, ранг/общее количество рестаранов, количество туристов в год.\n",
    "2. Найти признаки, по которым распределение с рейтингом будет иметь похожее на город/рейтинг для объединения в группы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как новые признаки связаны со старыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(20, 10))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"citizens\", y=\"Ranking\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"tourists_per_year\", y=\"Number of Reviews\", ax=ax[1])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"citizens\", y=\"restaurants_number_TA\", ax=ax[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_boxplot_2('country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение сгенерированного признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_rev_delta'].hist()\n",
    "data['date_rev_delta'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явно есть выбросы. Обработаем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию для расчета границ выбросов для всех данных\n",
    "iqr_analysis(data['date_rev_delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, сколько записей содержит отзывы, где дельта между отзывами более года. Это чуть больше верхней границы по IQR\n",
    "data[data['date_rev_delta'] > 365]['date_rev_delta'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим значение на 365*3 для выбросов (выбрано экспериментально)\n",
    "data['date_rev_delta'] = data['date_rev_delta'].apply(\n",
    "    lambda x: 1095 if x > 1095 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим NA на среднее\n",
    "data['date_rev_delta'].fillna(data['date_rev_delta'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как изменилось распределение признака после замен.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_rev_delta'].hist()\n",
    "data['date_rev_delta'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date_rev_from_max'].hist()\n",
    "data['date_rev_from_max'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используем функцию для расчета границ выбросов для всех данных\n",
    "iqr_analysis(data['date_rev_from_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, сколько записей содержат данные, где признак больше верхней границы\n",
    "data[data['date_rev_from_max'] > 1132]['date_rev_from_max'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим значение на 1132 для выбросов\n",
    "data['date_rev_from_max'] = data['date_rev_from_max'].apply(\n",
    "    lambda x: 1132 if x > 1132 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пропуски заменим средним\n",
    "data['date_rev_from_max'].fillna(\n",
    "    data['date_rev_from_max'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признак Cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cuisine_num'].value_counts(ascending=True).plot(kind='barh')\n",
    "data['cuisine_num'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поссмотрим зависимость целевой переменной и количеству кухонь на тестовой части выборки\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"cuisine_num\", y=\"Rating\", ax=ax[0])\n",
    "sns.scatterplot(data=data[data['sample'] == 1],\n",
    "                x=\"cuisine_num\", y=\"Number of Reviews\", ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuisine_count.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на распределение рейтингов по кухням на датафрейме, где отработал explode (cuisine_count)\n",
    "fig, ax = plt.subplots(figsize=(60, 4))\n",
    "sns.boxplot(x='Cuisine Style', y='Rating',\n",
    "            data=cuisine_count[cuisine_count['sample'] == 1],\n",
    "            ax=ax)\n",
    "plt.xticks(rotation=45)\n",
    "ax.set_title('Boxplot для cuisines')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдение**: у категории кухонь, отмеченных на TA, как Dietary Restrictions рейтинги выше. \n",
    "Можно отметить рестараны отдельным признаком, если такие опции у него имеются.\n",
    "Какие опции включаем:\n",
    "* Vegetarian Friendly\n",
    "* Vegan Options\n",
    "* Halal\n",
    "* Kosher\n",
    "* Gluten Free Options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "\n",
    "### 4.2  Распределение целевой переменной Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Rating'].value_counts(ascending=True).plot(kind='barh')\n",
    "df_train['Rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Rating'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рейтинги распределены от 1 до 5 с шагом в 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "\n",
    "### 4.3 Корреляция признаков\n",
    "Проведем корреляционный анализ.\n",
    "\n",
    "Удалим из анализа:\n",
    "* Категориальные признаки (кухни, города)\n",
    "* sample (служебный признак)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сфорсмруем список признаков, которые исключаем из корреляционного анализа\n",
    "cols_to_drop = ['sample', 'city_copies',  'City_Amsterdam',  'City_Athens',  'City_Barcelona', 'City_Berlin',  'City_Bratislava',  'City_Brussels',  'City_Budapest',  'City_Copenhagen',  'City_Dublin',  'City_Edinburgh',  'City_Geneva',  'City_Hamburg',  'City_Helsinki', 'City_Krakow',  'City_Lisbon',  'City_Ljubljana',  'City_London',  'City_Luxembourg',  'City_Lyon',  'City_Madrid',  'City_Milan',  'City_Munich',  'City_Oporto',  'City_Oslo',  'City_Paris', 'City_Prague',  'City_Rome',  'City_Stockholm',  'City_Vienna',  'City_Warsaw',  'City_Zurich',  'City_nan', '',  'Afghani',  'African',  'Albanian',  'American',  'Arabic',  'Argentinean', 'Armenian',  'Asian',  'Australian',  'Austrian',  'Azerbaijani',  'Balti',  'Bangladeshi',  'Bar',  'Barbecue',  'Beer restaurants',  'Belgian',  'Brazilian',  'Brew Pub',  'British',  'Burmese',  'Cafe',  'Cajun & Creole',  'Cambodian',  'Campania',  'Canadian',  'Caribbean',  'Catalan',  'Caucasian',  'Central American',  'Central Asian',  'Central European',  'Central-Italian',  'Chilean',  'Chinese',  'Colombian',  'Contemporary',  'Croatian',  'Cuban',  'Czech',  'Danish',  'Deli',  'Delicatessen',  'Diner',  'Dining bars',  'Dutch',  'Eastern European',  'Ecuadorean',  'Egyptian',  'Emilian',  'Ethiopian',\n",
    "                'European', 'Fast Food',  'Filipino',  'French',  'Fruit parlours',  'Fujian',  'Fusion',  'Gastropub',  'Georgian',  'German',  'Gluten Free Options',  'Greek',  'Grill',  'Halal',  'Hawaiian',  'Healthy',  'Hungarian',  'Indian',  'Indonesian',  'International',  'Irish',  'Israeli',  'Italian',  'Jamaican',  'Japanese',  'Japanese Fusion',  'Korean',  'Kosher',  'Latin',  'Latvian',  'Lazio',  'Lebanese',  'Lombard',  'Malaysian',  'Mediterranean',  'Mexican',  'Middle Eastern',  'Minority Chinese',  'Mongolian',  'Moroccan',  'Native American',  'Neapolitan',  'Nepali',  'New Zealand',  'Northern-Italian',  'Norwegian',  'Pakistani',  'Persian',  'Peruvian',  'Pizza',  'Polish',  'Polynesian',  'Portuguese',  'Pub',  'Romagna',  'Romana',  'Romanian',  'Russian',  'Salvadoran',  'Sardinian',  'Scandinavian',  'Scottish',  'Seafood',  'Sicilian',  'Singaporean',  'Slovenian',  'Soups',  'South American',  'Southern-Italian',  'Southwestern',  'Spanish',  'Sri Lankan',  'Steakhouse',  'Street Food',  'Sushi',  'Swedish',  'Swiss',  'Taiwanese',  'Thai',  'Tibetan',  'Tunisian',  'Turkish',  'Tuscan',  'Ukrainian',  'Uzbek',  'Vegan Options',  'Vegetarian Friendly',  'Venezuelan',  'Vietnamese',  'Welsh',  'Wine Bar',  'Xinjiang',  'Yunnan',  'Unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Построим матрицу корреляций\n",
    "plt.figure(figsize=(30, 15))\n",
    "heatmap = sns.heatmap(data[data['sample'] == 1].drop(\n",
    "    cols_to_drop, axis=1).corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Матрица корреляций', fontdict={'fontsize': 18}, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .Подсветим те значения, где коэффициент корреляции больше заданного порога\n",
    "plt.figure(figsize=(30, 15))\n",
    "heatmap = sns.heatmap(abs(data[data['sample'] == 1].drop(\n",
    "    cols_to_drop, axis=1).corr()) > 0.8, vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Матрица корреляций, где корреляция > 0.8',\n",
    "                  fontdict={'fontsize': 18}, pad=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на корреляцию признаков с целевой переменной Rating, отсортируем\n",
    "plt.figure(figsize=(8, 12))\n",
    "heatmap = sns.heatmap(data[data['sample'] == 1].drop(\n",
    "    cols_to_drop, axis=1).corr()[['Rating']].sort_values(by='Rating', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "heatmap.set_title('Корреляция признаков с Rating',\n",
    "                  fontdict={'fontsize': 18}, pad=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем список признаков, которые коллинеарны.\n",
    "\n",
    "Для этого выставим критерий наличия корреляции больше 0.8 или -0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сформируем сет со скоррелированными признаками\n",
    "correlated_features = set()\n",
    "# Удаляем целевую переменную из матрицы коррелиций, тк корреляция с ней, - хорошо для модели\n",
    "correlation_matrix = data[data['sample'] == 1].drop(\n",
    "    ['Rating', 'sample'], axis=1).corr()\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[j]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "print('Список скоррелированных признаков на удаление из обучения модели:',\n",
    "      correlated_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4\"></a>\n",
    "\n",
    "### 4.4 Поиск статистически значимых различий с помощью теста Стьюдента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графики являются лишь вспомогательным инструментом, настоящую значимость различий может помочь распознать статистика. \n",
    "\n",
    "Проверим, есть ли статистическая разница в распределении оценок по номинативным признакам, с помощью **теста Стьюдента**. Проверим нулевую гипотезу о том, что распределения рейтинга по различным параметрам неразличимы.\n",
    "\n",
    "Анализ будем проводить для колонок, которые номинативные по типу данных, но и для колонок-шпионов, которые количественные, но обозначают номинативный признак (принадлежность к городу, типу кухни)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stat_dif_2(column):\n",
    "    \"\"\" \n",
    "    Поиск статистически значимых различий для колонки с помощью теста Стьюдента.\n",
    "    \"\"\"\n",
    "    cols = data[data['sample'] == 1].loc[:, column].value_counts().index[:]\n",
    "    combinations_all = list(combinations(cols, 2))\n",
    "    # Тест проводим на изначальном наборе данных без NA значений для целевого столбца, столбца с признаком, дополнительно исключив 0 для оценок\n",
    "    stud_stat = data[data['sample'] == 1]\n",
    "    for comb in combinations_all:\n",
    "        if ttest_ind(stud_stat.loc[data[data['sample'] == 1].loc[:, column] == comb[0], 'Rating'],\n",
    "                     stud_stat.loc[data[data['sample'] == 1].loc[:, column] == comb[1], 'Rating']).pvalue <= 0.05/len(combinations_all):  # учли поправку Бонферони\n",
    "            # print('Найдены статистически значимые различия для колонки', column)\n",
    "            pass\n",
    "        else:\n",
    "            return column\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сформируем сет для статистически незначимых признаков\n",
    "to_remove_features = set()\n",
    "\n",
    "# Проходим по колонкам, которые исключали из корреляционного анализа\n",
    "for column in cols_to_drop:\n",
    "    to_remove_features.add(get_stat_dif_2(column))\n",
    "\n",
    "print('\\n Список признаков на удаление из обучения модели:', to_remove_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединяем списки колонок на удаление из анализа выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формируем сет, конвертируем в список, удаляем NAN\n",
    "drop_features = correlated_features.union(to_remove_features)\n",
    "drop_features = list(drop_features)\n",
    "drop_features.remove(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id=\"5\"></a>\n",
    "\n",
    "# 5. DATA PREPROCESSING\n",
    "Теперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На всякий случай, заново подгружаем данные\n",
    "DATA_DIR = '/kaggle/input/sf-dst-restaurant-rating/'\n",
    "df_train = pd.read_csv(DATA_DIR+'/main_task.csv')\n",
    "df_test = pd.read_csv(DATA_DIR+'/kaggle_task.csv')\n",
    "sample_submission = pd.read_csv(DATA_DIR+'sample_submission.csv')\n",
    "cities_info = pd.read_csv('/kaggle/input/citiesdata-2/cities_data.csv')\n",
    "\n",
    "# Создадим датафрейм data_ta из данных, которые собрали с TripAdvisor\n",
    "# Открываем каждый файл и добавляем его в data_ta\n",
    "data_ta = pd.DataFrame()\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/data-ta/'):\n",
    "    for filename in filenames:\n",
    "        temp = pd.read_csv(os.path.join(dirname, filename))\n",
    "        data_ta = pd.concat([data_ta, temp], ignore_index=True)\n",
    "\n",
    "df_train['sample'] = 1  # помечаем где у нас трейн\n",
    "df_test['sample'] = 0  # помечаем где у нас тест\n",
    "# в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "df_test['Rating'] = 0\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(\n",
    "    drop=True)  # объединяем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(df_input, data_ta_input):\n",
    "    '''Включены функции и операции по предобработке данных для модели.'''\n",
    "\n",
    "    df_output = df_input.copy()\n",
    "    data_ta = data_ta_input.copy()\n",
    "\n",
    "    #################### 1. Предобработка ##############################################################\n",
    "    # Соберем здесь используемые функции, обработку данных.\n",
    "\n",
    "    def haversine(lon1, lat1, lon2, lat2):\n",
    "        '''\n",
    "        Функция принимает на вход координаты города и ресторана. \n",
    "        На выходе возвращает расстояние от центра города до ресторана.\n",
    "        '''\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1\n",
    "        dlat = lat2 - lat1\n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2) ** 2\n",
    "        c = 2 * asin(sqrt(a))\n",
    "        earth_radius = 6371  # in km\n",
    "        return c * earth_radius\n",
    "\n",
    "    def clean_name(str_val):\n",
    "        \"\"\"\n",
    "        Преобразует строку с названиями кухонь в список [list] названий кухонь.\n",
    "        На входе:\n",
    "            - строковая переменная, содержащая названия кухонь.\n",
    "        На выходе:\n",
    "            - список [list] названий кухонь.\n",
    "        \"\"\"\n",
    "        if pd.isna(str_val):\n",
    "            return ['Unknown']\n",
    "        str_val = str_val.strip('[]')  # Отбрасываем скобки\n",
    "        str_val = str_val.replace(\"\\'\", '')  # Убираем кавычки '\n",
    "        str_val = str_val.split(\", \")  # Разбиваем строку по названиям кухонь\n",
    "        return str_val\n",
    "\n",
    "    def cuisine_nan_replace(row):\n",
    "        '''\n",
    "        Функция на вход принимает строку датафрейма, проверяем ее значение.\n",
    "        На выход возвращает или изначальное значение списка кухонь, или соотвествующий список с TA для тех кухонь, где указано Unknown.\n",
    "        '''\n",
    "        if row['Cuisine Style'][0] == 'Unknown':\n",
    "            return row['cuisine_styles_ta']\n",
    "        else:\n",
    "            return row['Cuisine Style']\n",
    "\n",
    "    def clean_type(str_val):\n",
    "        \"\"\"\n",
    "        Преобразует строку с Unknown названием кухни в список [list].\n",
    "        На входе:\n",
    "            - колонка, содержащая названия кухонь.\n",
    "        На выходе:\n",
    "            - список [list] названий кухонь.\n",
    "        \"\"\"\n",
    "        if type(str_val) == str:\n",
    "            return str_val.split()\n",
    "        return str_val\n",
    "\n",
    "    def dietary_restrictions(row):\n",
    "        \"\"\"\n",
    "        Функция на вход принимает строку датафрейма.\n",
    "        Если в списке кухонь ресторана есть одна из кухонь списка спец. кухонь, то\n",
    "            - возвращаем 1\n",
    "            - иначе возвращаем 0.\n",
    "        \"\"\"\n",
    "        dietary_restrictions = ['Vegetarian Friendly', 'Vegan Options',\n",
    "                                'Gluten Free Options', 'Halal', 'Kosher']\n",
    "        for i in dietary_restrictions:\n",
    "            if i in row['Cuisine Style'] and i != '':\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def get_stat_dif_2(column):\n",
    "        \"\"\" \n",
    "        Поиск статистически значимых различий для колонки с помощью теста Стьюдента.\n",
    "        \"\"\"\n",
    "        cols = df_output[df_output['sample'] ==\n",
    "                         1].loc[:, column].value_counts().index[:]\n",
    "        combinations_all = list(combinations(cols, 2))\n",
    "        # Тест проводим на изначальном наборе данных без NA значений для целевого столбца, столбца с признаком, дополнительно исключив 0 для оценок\n",
    "        stud_stat = df_output[df_output['sample'] == 1]\n",
    "        for comb in combinations_all:\n",
    "            if ttest_ind(stud_stat.loc[df_output[df_output['sample'] == 1].loc[:, column] == comb[0], 'Rating'],\n",
    "                         stud_stat.loc[df_output[df_output['sample'] == 1].loc[:, column] == comb[1], 'Rating']).pvalue <= 0.05/len(combinations_all):  # учли поправку Бонферони\n",
    "                # print('Найдены статистически значимые различия для колонки', column)\n",
    "                pass\n",
    "            else:\n",
    "                return column\n",
    "                break\n",
    "\n",
    "    # Почистим формат колонок с ID (ID_TA, Restaurant_id), избавимся от префиксов\n",
    "    df_output.Restaurant_id = df_output.Restaurant_id.apply(\n",
    "        lambda x: int(x[3:]))\n",
    "    df_output.ID_TA = df_output.ID_TA.apply(lambda x: int(x[1:]))\n",
    "\n",
    "    # Обработка внешних данных с TA\n",
    "    # Удаляем ненужные колонки\n",
    "    # Для списка сертификатов будем использовать только имя сертификата\n",
    "    data_ta.drop([col for col in data_ta.columns if col.endswith(\n",
    "        '/year')], axis=1, inplace=True)\n",
    "    data_ta.drop([col for col in data_ta.columns if col.startswith(\n",
    "        'hours/')], axis=1, inplace=True)  # Время работы ресторана не используем\n",
    "    data_ta.drop(['address', 'phone', 'rankingPosition', 'type', 'webUrl', 'website', 'email',\n",
    "                  'isClosed', 'isLongClosed', 'rating'], axis=1, inplace=True)  # Доп список признаков, которые решила не использовать точно\n",
    "    # Переименуем колонку ID_TA для простоты последующего мержа\n",
    "    data_ta.rename(columns={\"id\": \"ID_TA\"}, inplace=True)\n",
    "    # Т.к. набор колонок у разных испточников данных разный, то отсортируем все полученные колонки по алфавиту\n",
    "    # для простоты работы и воспрозводимости кода\n",
    "    # переменная со списком отсортированных колонок\n",
    "    columns_list = list(data_ta.columns.sort_values())\n",
    "    data_ta = data_ta[columns_list]  # модифицируем датафрейм\n",
    "\n",
    "    # Создаем признак, который будет хранить все награды ресторана\n",
    "    data_ta['awards_ta'] = data_ta[data_ta.columns[1:12]].apply(\n",
    "        lambda x: ', '.join(x.dropna().astype(str)),\n",
    "        axis=1)  # Проходимся по колонкам с наградами, объединяем непустые значения в строку через запятую\n",
    "    data_ta['awards_ta'] = data_ta['awards_ta'].apply(\n",
    "        lambda x: x.split(\", \"))  # создаем список наград для каждого ресторана\n",
    "\n",
    "    # Создаем признак awards_num, который будет хранить количество наград у ресторана\n",
    "    len_cert_list = []\n",
    "    for i in range(0, len(data_ta)):\n",
    "        if data_ta['awards_ta'][i][0] == '':  # если список наград пустой, то записываем 0\n",
    "            len_cert_list.append(0)\n",
    "        else:\n",
    "            # если непустой, то записываем длину списка\n",
    "            len_cert_list.append(len(data_ta['awards_ta'][i]))\n",
    "    data_ta['awards_num'] = len_cert_list  # добавляем признак\n",
    "\n",
    "    # Создаем признак со списками кухонь, который будет хранить список кухонь дл] ресторана\n",
    "    data_ta['cuisine_styles_ta'] = data_ta[data_ta.columns[13:-9]].apply(\n",
    "        lambda x: ', '.join(x.dropna().astype(str)),\n",
    "        axis=1)  # Проходимся по колонкам с кухнями, объединяем непустые значения в строку через запятую\n",
    "    data_ta['cuisine_styles_ta'] = data_ta['cuisine_styles_ta'].apply(\n",
    "        lambda x: x.split(\", \"))  # создаем список кухонь для каждого ресторана\n",
    "\n",
    "    # Создаем датафрейм с колонками, которые хотим перенести в исходный датафрейм для модели data\n",
    "    data_ta_output = data_ta[['ID_TA', 'awards_num',\n",
    "                              'cuisine_styles_ta', 'longitude', 'latitude']]\n",
    "    # Удаляем дубликаты для ресторанов (такие есть) для корректного мержа\n",
    "    data_ta_output.drop_duplicates(subset=['ID_TA'], inplace=True)\n",
    "\n",
    "    # Смержим рабочий датафрейм с внешними данными из TA\n",
    "    df_output = pd.merge(df_output, data_ta_output, on=\"ID_TA\",\n",
    "                         how=\"left\")  # объединяем по ID_TA\n",
    "\n",
    "    # Мержим рабочий датафрейм с внешними данными по городам\n",
    "    df_output = pd.merge(df_output, cities_info, on=\"City\",\n",
    "                         how=\"left\")  # объединяем по City\n",
    "\n",
    "    # Создадим признак с копией городов перед дамми-кодированием, т.к. изначальная колонка может быть полезной.\n",
    "    df_output['city_copies'] = df_output['City']\n",
    "\n",
    "    # Применим ф-ию по чистке данных для кухонь\n",
    "    df_output[\"Cuisine Style\"] = df_output[\"Cuisine Style\"].apply(clean_name)\n",
    "\n",
    "    # Создадим датафрейм, в который запишем суммы количества ревью по городам\n",
    "    reviews_sum = pd.DataFrame(df_output.groupby(['city_copies'])[\n",
    "        'Number of Reviews'].sum().sort_values(ascending=False))\n",
    "    reviews_sum.rename(\n",
    "        columns={\"Number of Reviews\": \"ttl_reviews_per_city\"}, inplace=True)\n",
    "    # Смержим созданный датафрейм с исходным датафреймам по городу\n",
    "    df_output = pd.merge(df_output, reviews_sum, on=\"city_copies\", how=\"left\")\n",
    "\n",
    "    # ################### 2. NAN ##############################################################\n",
    "    # Заполним пропуски Price Range модой\n",
    "    df_output['Price Range'].fillna(\n",
    "        df_output['Price Range'].mode()[0], inplace=True)\n",
    "\n",
    "    # Заполняем медианой NA в awards_num\n",
    "    df_output.awards_num.fillna(df_output.awards_num.median(), inplace=True)\n",
    "\n",
    "    # Заполняем данными про типы кухонь с TA с помощью функции\n",
    "    df_output['Cuisine Style'] = df_output.apply(cuisine_nan_replace, axis=1)\n",
    "    # Заполняем значением Unknown, тк не все данные были на TA\n",
    "    df_output['Cuisine Style'].fillna(\"Unknown\", inplace=True)\n",
    "    # Строки с Unknown типа str, а остальные - list. Сделаем преобразования.\n",
    "    df_output[\"Cuisine Style\"] = df_output[\"Cuisine Style\"].apply(clean_type)\n",
    "\n",
    "    # Создаем новый признак до заполнения пропусков\n",
    "    df_output['number_of_rev_is_NAN'] = pd.isna(\n",
    "        df_output['Number of Reviews']).astype('uint8')\n",
    "    # Заполняем пропуски Number of Reviews медианой по городу\n",
    "    median_reviews = df_output.groupby(\n",
    "        ['City'])['Number of Reviews'].median()  # series с медианами по городам\n",
    "    df_output['Number of Reviews'] = df_output.apply(lambda x: median_reviews.loc[x['City']] if pd.isna(\n",
    "        x['Number of Reviews']) else x['Number of Reviews'], axis=1)\n",
    "\n",
    "    # Заменим выбросы в датафрейме data на максимальное пограничное знаечение признака\n",
    "    df_output['Number of Reviews'] = df_output['Number of Reviews'].apply(\n",
    "        lambda x: 840 if x >= 840 else x)\n",
    "\n",
    "    # В тестовой выборке есть пустые значения, заменим их на строку, которая показывает, что ревью нет.\n",
    "    df_output['Reviews'].fillna('[[], []]', inplace=True)\n",
    "\n",
    "    # ################### 3. Encoding ##############################################################\n",
    "    # Используем One-Hot Encoding в pandas - get_dummies для кодирования городов.\n",
    "    df_output = pd.get_dummies(df_output, columns=['City', ], dummy_na=True)\n",
    "\n",
    "    # Используем MultiLabelBinarizer() для кодирования cписка кухонь\n",
    "    s = df_output['Cuisine Style']\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    cuisine_df = pd.DataFrame(mlb.fit_transform(\n",
    "        s), columns=mlb.classes_, index=data.index)  # cсоздаем датафрейм с дамми кухнями\n",
    "    # Смержим рабочий датафрейм с датафреймом дамми-кухонь\n",
    "    df_output = df_output.merge(cuisine_df, left_index=True, right_index=True)\n",
    "\n",
    "    # ################### 4. Feature Engineering ####################################################\n",
    "    # Создаем новый признак distance, который будет показывать расстояние от центра города до ресторана.\n",
    "    df_output['distance'] = df_output.apply(lambda row:\n",
    "                                            haversine(lon1=row['lon_c'],\n",
    "                                                      lat1=row['lat_c'],\n",
    "                                                      lon2=row['longitude'],\n",
    "                                                      lat2=row['latitude']),\n",
    "                                            axis=1)\n",
    "    # Заполняем пропуски значением среднего по городу\n",
    "    mean_distance = df_output.groupby(['city_copies'])['distance'].mean()\n",
    "    df_output['distance'] = df_output.apply(lambda x: mean_distance.loc[x['city_copies']] if pd.isna(\n",
    "        x['distance']) else x['distance'], axis=1)\n",
    "\n",
    "    # Создаем новый признак с использованием внешних данных по городам\n",
    "    # reviews_per_ttl_ppl - показывает сколько ревью приходится на суммарное 1000 людей (жители + туристы)\n",
    "    df_output['reviews_per_ttl_ppl'] = df_output.apply(lambda row: (\n",
    "        row['Number of Reviews']/(row['citizens']+row['tourists_per_year']))*1000, axis=1)\n",
    "\n",
    "    # Price Range - переведем в цифровые значения\n",
    "    pricerange_dict = {\"nan\": 0, \"$\": 1, \"$$ - $$$\": 2, \"$$$$\": 3}\n",
    "    df_output['price_range_num'] = df_output['Price Range']\n",
    "    df_output['price_range_num'].replace(\n",
    "        to_replace=pricerange_dict, inplace=True)  # заменяем значения в соответствии со словарем\n",
    "\n",
    "    # Добавляем признак cuisine_num\n",
    "    len_cuisines_list = []\n",
    "    for i in range(0, len(df_output)):\n",
    "        if df_output['Cuisine Style'][i][0] == 'Unknown':\n",
    "            len_cuisines_list.append(-1)  # -1 для пропуско\n",
    "        elif df_output['Cuisine Style'][i][0] == '':\n",
    "            len_cuisines_list.append(0)  # 0, где кухонь нет и на TA\n",
    "        else:\n",
    "            len_cuisines_list.append(len(df_output['Cuisine Style'][i]))\n",
    "    df_output['cuisine_num'] = len_cuisines_list\n",
    "\n",
    "    # Создаем признак dietary_restrictions\n",
    "    df_output['dietary_restrictions'] = df_output.apply(\n",
    "        dietary_restrictions, axis=1)\n",
    "\n",
    "    # Создадим новый признак review_date на основе патерна поиска дат.\n",
    "    pattern = re.compile('\\d+\\/\\d+\\/\\d+')\n",
    "    df_output['review_date'] = df_output.Reviews.apply(pattern.findall)\n",
    "    # Чистка данных, где в поле review_date попали даты-упоминания из комментариев отзыва.\n",
    "    df_output.review_date = df_output.review_date.apply(\n",
    "        lambda x: [x[-2], x[-1]] if len(x) > 2 else x)\n",
    "\n",
    "    # Создаем новые признаки, сразу переводим в формат datetime64\n",
    "    df_output['date_rev_1'] = pd.to_datetime(\n",
    "        df_output.review_date.apply(lambda x: x[0] if len(x) >= 1 else None))\n",
    "    df_output['date_rev_2'] = pd.to_datetime(\n",
    "        df_output.review_date.apply(lambda x: x[1] if len(x) >= 2 else None))\n",
    "    df_output['date_rev_delta'] = (\n",
    "        abs(df_output.date_rev_2-df_output.date_rev_1)) / np.timedelta64(1, \"D\")\n",
    "\n",
    "    # Создаем новый признак про актуальность отзывов\n",
    "    date_max = df_output[['date_rev_1', 'date_rev_2']].max(axis=1).max()\n",
    "    df_output['date_rev_from_max'] = df_output.apply(lambda row: None if len(row.review_date) == 0  # если пустые значения, то Nan\n",
    "                                                     # если одна дата, то смотрим разницу с первым отзывом\n",
    "                                                     else (date_max-row.date_rev_1) if len(row.review_date) == 1\n",
    "                                                     else ((date_max-row.date_rev_2)), axis=1) / np.timedelta64(1, \"D\")  # если два отзыва, то берем второй отзыв\n",
    "\n",
    "    # Заменим значение на 365*3 для выбросов (выбрано экспериментально)\n",
    "    df_output['date_rev_delta'] = df_output['date_rev_delta'].apply(\n",
    "        lambda x: 1095 if x > 1095 else x)\n",
    "    # Заменим NA на среднее\n",
    "    df_output['date_rev_delta'].fillna(\n",
    "        df_output['date_rev_delta'].mean(), inplace=True)\n",
    "\n",
    "    # Заменим значение на 1132 для выбросов (верхняя граница по IQR)\n",
    "    df_output['date_rev_from_max'] = df_output['date_rev_from_max'].apply(\n",
    "        lambda x: 1132 if x > 1132 else x)\n",
    "    # Пропуски заменим средним\n",
    "    df_output['date_rev_from_max'].fillna(\n",
    "        df_output['date_rev_from_max'].mean(), inplace=True)\n",
    "\n",
    "    # Создадим признак о том, что ресторан сетевой\n",
    "    # Сначала найдем ID ресторанов, у которых в value_counts более одного ресторана, сохраним список\n",
    "    in_chain_index = df_output['Restaurant_id'].value_counts(\n",
    "    ).loc[lambda x: x > 1].index\n",
    "    df_output['in_chain'] = df_output['Restaurant_id'].apply(\n",
    "        lambda x: 1 if x in in_chain_index else 0)\n",
    "\n",
    "    # Создаем признак rank_per_ttl\n",
    "    # rank_per_ttl - показывает относительную позицию ранга ресторана к общему количеству рангов по городу.\n",
    "    df_output['rank_per_ttl'] = df_output.apply(\n",
    "        lambda x: x['Ranking']/x['restaurants_number_TA'], axis=1)\n",
    "\n",
    "    # Добавление признаков перемножением\n",
    "    df_output[\"ranking_num_reviews\"] = df_output[\"Ranking\"] * \\\n",
    "        df_output[\"Number of Reviews\"]\n",
    "    df_output[\"ranking_num_cuisines\"] = df_output[\"Ranking\"] * \\\n",
    "        df_output[\"cuisine_num\"]\n",
    "\n",
    "    # Создаем новый признак reviews_perc_in_city_ttl\n",
    "    # reviews_perc_in_city_ttl - отношения количества ревью ресторана к суммарному количеству ревью по городу из выборки\n",
    "    df_output['reviews_perc_in_city_ttl'] = df_output.apply(\n",
    "        lambda x: x['Number of Reviews']/x['ttl_reviews_per_city'], axis=1)\n",
    "\n",
    "    # ################### 5. Clean ####################################################\n",
    "    # Удаляем признаки, которые не отобрали для модели во время анализа\n",
    "    # Сфорсмруем список признаков, которые исключаем из корреляционного анализа\n",
    "    cols_to_drop = ['sample', 'city_copies',  'City_Amsterdam',  'City_Athens',  'City_Barcelona', 'City_Berlin',  'City_Bratislava',  'City_Brussels',  'City_Budapest',  'City_Copenhagen',  'City_Dublin',  'City_Edinburgh',  'City_Geneva',  'City_Hamburg',  'City_Helsinki', 'City_Krakow',  'City_Lisbon',  'City_Ljubljana',  'City_London',  'City_Luxembourg',  'City_Lyon',  'City_Madrid',  'City_Milan',  'City_Munich',  'City_Oporto',  'City_Oslo',  'City_Paris', 'City_Prague',  'City_Rome',  'City_Stockholm',  'City_Vienna',  'City_Warsaw',  'City_Zurich',  'City_nan', '',  'Afghani',  'African',  'Albanian',  'American',  'Arabic',  'Argentinean', 'Armenian',  'Asian',  'Australian',  'Austrian',  'Azerbaijani',  'Balti',  'Bangladeshi',  'Bar',  'Barbecue',  'Beer restaurants',  'Belgian',  'Brazilian',  'Brew Pub',  'British',  'Burmese',  'Cafe',  'Cajun & Creole',  'Cambodian',  'Campania',  'Canadian',  'Caribbean',  'Catalan',  'Caucasian',  'Central American',  'Central Asian',  'Central European',  'Central-Italian',  'Chilean',  'Chinese',  'Colombian',  'Contemporary',  'Croatian',  'Cuban',  'Czech',  'Danish',  'Deli',  'Delicatessen',  'Diner',  'Dining bars',  'Dutch',  'Eastern European',  'Ecuadorean',  'Egyptian',  'Emilian',  'Ethiopian',\n",
    "                    'European', 'Fast Food',  'Filipino',  'French',  'Fruit parlours',  'Fujian',  'Fusion',  'Gastropub',  'Georgian',  'German',  'Gluten Free Options',  'Greek',  'Grill',  'Halal',  'Hawaiian',  'Healthy',  'Hungarian',  'Indian',  'Indonesian',  'International',  'Irish',  'Israeli',  'Italian',  'Jamaican',  'Japanese',  'Japanese Fusion',  'Korean',  'Kosher',  'Latin',  'Latvian',  'Lazio',  'Lebanese',  'Lombard',  'Malaysian',  'Mediterranean',  'Mexican',  'Middle Eastern',  'Minority Chinese',  'Mongolian',  'Moroccan',  'Native American',  'Neapolitan',  'Nepali',  'New Zealand',  'Northern-Italian',  'Norwegian',  'Pakistani',  'Persian',  'Peruvian',  'Pizza',  'Polish',  'Polynesian',  'Portuguese',  'Pub',  'Romagna',  'Romana',  'Romanian',  'Russian',  'Salvadoran',  'Sardinian',  'Scandinavian',  'Scottish',  'Seafood',  'Sicilian',  'Singaporean',  'Slovenian',  'Soups',  'South American',  'Southern-Italian',  'Southwestern',  'Spanish',  'Sri Lankan',  'Steakhouse',  'Street Food',  'Sushi',  'Swedish',  'Swiss',  'Taiwanese',  'Thai',  'Tibetan',  'Tunisian',  'Turkish',  'Tuscan',  'Ukrainian',  'Uzbek',  'Vegan Options',  'Vegetarian Friendly',  'Venezuelan',  'Vietnamese',  'Welsh',  'Wine Bar',  'Xinjiang',  'Yunnan',  'Unknown']\n",
    "    # Сформируем сет со скоррелированными признаками\n",
    "    correlated_features = set()\n",
    "    # Удаляем целевую переменную из матрицы коррелиций, тк корреляция с ней, - хорошо для модели\n",
    "    correlation_matrix = df_output[df_output['sample'] == 1].drop(\n",
    "        ['Rating', 'sample'], axis=1).corr()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "                colname = correlation_matrix.columns[j]\n",
    "                correlated_features.add(colname)\n",
    "\n",
    "    # Сформируем сет для статистически незначимых признаков\n",
    "    to_remove_features = set()\n",
    "    # Проходим по колонкам, которые исключали из корреляционного анализа\n",
    "    for column in cols_to_drop:\n",
    "        to_remove_features.add(get_stat_dif_2(column))\n",
    "\n",
    "    # Формируем сет, конвертируем в список, удаляем NAN\n",
    "    drop_features = correlated_features.union(to_remove_features)\n",
    "    drop_features = list(drop_features)\n",
    "    drop_features.remove(None)\n",
    "\n",
    "    df_output.drop(drop_features, axis=1, inplace=True)\n",
    "\n",
    "    # Модель на признаках с dtypes \"object\" обучаться не будет, просто выберим их и удалим\n",
    "    object_columns = [\n",
    "        s for s in df_output.columns if df_output[s].dtypes in ['object', '<M8[ns]']]\n",
    "    df_output.drop(object_columns, axis=1, inplace=True)\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запускаем и проверяем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preproc = preproc_data(data, data_ta)\n",
    "df_preproc.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь выделим тестовую часть\n",
    "train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем, как отправлять  данные на обучение, разделим данные на еще один тест и трейн, для валидации. \n",
    "\n",
    "Это поможет проверить, как хорошо модель работает, до отправки submissiona на kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "# 6. MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки:\n",
    "# инструмент для создания и обучения модели\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics  # инструменты для оценки точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как признак рейтинга имеет шаг 0.5, округляем предсказание.\n",
    "y_pred = np.round(y_pred * 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "print('MAE:', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "source": [
    "Стартовое значение MAE: 0.428\n",
    "\n",
    "Прогресс: \n",
    "* 0.214 (добавление осн. признаков)\n",
    "* 0.2131 (добавление дамми инфы про кухни)\n",
    "* 0.304 (если удалить Ranking и добавить ranking_quantile)\n",
    "* 0.2186 (если оставить и Ranking, и ranling_quantile)\n",
    "* 0.2186 (убирание выбросов по Number of Reviews никак не повлияло)\n",
    "* 0.2186 (после нормализации ранга – без влияния)\n",
    "* 0.213034375 (закомментировала ranling_quantile, улучшение)\n",
    "* 0.21845 (public) - 0.213 - **440** место – пробный сабмит\n",
    "* 0.20 - rank_per_ttl (v. 33) - **317** место - 0.21177\n",
    "* 0.206870625 - добавила in_chain\n",
    "* 0.207 - убрала выбросы в Num of Review и стало хуже\n",
    "* v.34 - 0.207178125, 0.21202 (стало хуже) - **318** место - добавила выбросы по delta-reviews\n",
    "* v.35 + ranking_num_reviews, reviews_per_ttl_ppl, ranking_num_cuisines (0.20595999999999998) **317** 0.21161\n",
    "* v.36: подкорректировала границы выбросов, чтоб задеть меньше данных (0.20570312499999996)\n",
    "* v.38: вернула признак is_Nan для ревью 0.20369375 (0.20880)\n",
    "* v.39: добавила признак dietary_restrictions, локально ухудшила результат.\n",
    "*  добавила date_rev_from_max 0.199690625 **0.20441** **233 место**\n",
    "* v.40: reviews_perc_in_city_ttl 0.19911312499999997 **0.20427 233 место**\n",
    "* v. 42: awards_num с TA: 0.196016875, **0.20106 223 место**\n",
    "* v. 43: cousines from TA: 0.19679624999999998, **0.20123, 233 место** стало хуже!\n",
    "* Округление шага в 0.5 0.1659375 0.17125 **0.17125, 65 место**\n",
    "* Добавила координаты широты и долготы, поправила обработку кол-ва кухонь (на -1) 0.165875\n",
    "* Заменила -1 на среднее по признаку 0.16625 **0.17255** хуже\n",
    "* Откатила изменение **0.17270** Т.е. добавление широты и долготы не улучшает результат.\n",
    "* v. 44 Отбор признаков **0.160125** **0.16695 29 место**\n",
    "* Заполнение distance, 0.161875 хуже, но лучше в финалке **0.16675** 28 место\n",
    "* 0.161 исправила баг с кухнями  0.16810\n",
    "*  0.161937 - выбросы эксперименты\n",
    "* v.45: 0.161  0.16810 - 28 место, финалка без «причесывания» кода\n",
    "* v. 49 0.1595625 - Your submission scored 0.16810 - 28 место.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(20).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "\n",
    "# 7. SUBMISSION \n",
    "Готовим Submission на Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так как признак рейтинга имеет шаг 0.5, округляем предсказание.\n",
    "predict_submission = np.round(predict_submission * 2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "\n",
    "# 8. SUMMARY \n",
    "\n",
    "По ходу выполнения проекта:\n",
    "1. Были добавлены внешние данные с информацией по городам и из TA\n",
    "2. Были добавлены следующие признаки:\n",
    "| Признак | Описание |\n",
    "|-: |:- |\n",
    "| number_of_rev_is_NAN | Наличие пропусков в изначальных данных по количеству отзывов | \n",
    "| awards_ta | Список наград ресторана с TA| \n",
    "| awards_num | Количество наград у ресторана с TA | \n",
    "| cuisine_styles_ta | Cписок с кухнями ресторана с TA| \n",
    "| longitude | Географические координаты ресторана с TA | \n",
    "| latitude | Географические координаты ресторана с TA|\n",
    "| country  | Страна, в которой находится город |\n",
    "| citizens | Население города, чел |\n",
    "| restaurants_number_TA | Количество ресторанов, участвующих в рейтинге |\n",
    "| citizens_per_restaurant | Количество горожан на один ресторан |\n",
    "| tourists_per_year| Количество туристов, посетивших город в течение года, чел |\n",
    "| ttl_ppl_per_restaurants | (Количество туристов + население города) / количество ресторанов |\n",
    "| distance  | Расстояние от центра города до ресторана |\n",
    "| reviews_per_ttl_ppl | Показывает, сколько ревью приходится на суммарных 1000 людей (жители + туристы) |\n",
    "| ttl_reviews_per_city | Суммарное количество ревью по городу из выборки |\n",
    "| reviews_perc_in_city_ttl | Отношения количества ревью ресторана к суммарному количеству ревью по городу из выборки |\n",
    "| price_range_num | Ценовая категория ресторана: 1, 2, 3 |\n",
    "| cuisine_num | Количество типов кухонь ресторана |\n",
    "| dietary_restrictions | Наличие у ресторана спец. опций по кухням |\n",
    "| review_date | Все даты ревью |\n",
    "| date_rev_1 | Дата первого ревью |\n",
    "| date_rev_2 | Дата второго ревью |\n",
    "| in chain | Показатель, сетевой ли ресторан | \n",
    "| rank_per_ttl | показывает относительную позицию ранга ресторана к общему количеству рангов по городу |\n",
    "| ranking_num_reviews | Умножение Ranking и Number of Reviews |\n",
    "| ranking_num_cuisines | Умножение Ranking и cuisine_num |\n",
    "| date_rev_delta | Количество дней между оставленными ревью |\n",
    "| date_rev_from_max | Количество дней от последнего отзыва до самого свежего отзыва в датасете |\n",
    "| City | Созданы 31 признак для кодировки города|\n",
    "| Cuisine Style | Созданы 146 признака для кодировки типа кухни, включая отсуствие признака или пропуск |\n",
    "3. После отбора признаков для модели были получены резултат:\n",
    "    * Локальное MAE: 0.1595625\n",
    "    * MAE на Kaggel (для submission): 0.16810 (28 место на момент отправки)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
